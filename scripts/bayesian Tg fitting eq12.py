#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri May 19 16:57:36 2023

@author: James H. Merrill, Department of Physics, Emory University
"""

from IPython.core.pylabtools import figsize
import numpy as np
from matplotlib import pyplot as plt
from tkinter import *
from tkinter import filedialog
import pymc as pm
import arviz as az
import scipy.stats as stats
import xarray as xr
import time as t
import pandas as pd
import sys
import os
import arviz.labels as azl

 
def lncosh_bayes_fit(fname, draw=2000, tune=1000, T_max = 130, plot_ppc = True, save_data = True, save_summary = True, return_data = True, fit_error = True):
    """
    

    Parameters
    ----------
    fname : filename or file handle
        Filename for the dataset to fit.        
    draw : int, optional
        The number of samples to draw. The default is 2000.
    tune : int, optional
        Number of samples to use for tuning the adaptive step length for NUTS. The default is 1000.
    plot_ppc : bool, optional
        Choose whether the generate posterior predictive samples, which generates simulated data from the posterior distributions of each param in the model, should be sampled and plotted. The default is True.
    save_data : bool, optional
        Choose whether to save the inference data generated by pymc. The default is True.
    save_summary : bool, optional
        Choose whether to save the text summary of the inference performed. The default is True.
    return_data : bool, optional
        Choose whether to return the raw data that was fit.
    fit_error : bool, optional
        Choose whether to fit the noise in the data as a parameter, or not. The default is False. WARNING: SETTING TO TRUE WILL MAKE ABSOLUTE VALUES OF FIT PARAMETER ERRORS UNPHYSICAL.
    Returns
    -------
    trace : pymc.backends.base.MultiTrace or arviz.InferenceData
        A ``MultiTrace`` or ArviZ ``InferenceData`` object that contains the samples.
    count_data : array-like, optional
        Unscaled film thickness or intensity
    T : array-like, optional
        Unscaled temperature array

    """
    dat = np.loadtxt(fname, skiprows = 1, unpack = False)
    count_data = dat.transpose()[1]
    T = dat.transpose()[0]
    h_err = dat.transpose()[2]
    target_accept = 0.8 #same as default value for NUTS
    
        
    
    n_count_data = len(count_data)
    x_n = stats.zscore(T) #standardize T
    y = count_data 
    y_n = stats.zscore(y) # standardize h
    with pm.Model() as model:
        
        #define priors: initial guess for shape and initial values for probability distributions of each variable in model
        if fit_error is False:
            sigma = np.mean(h_err)/np.std(y)
        else:
            sigma = pm.HalfCauchy("Sigma", beta = .1)
        #sigma = 0.0080
        intercept = pm.Normal("Intercept", mu=0, sigma=1)
        M = pm.Normal("Melt linear expansion coefficient", mu=0, sigma=1) #(M+G0)/2
        G_0 = pm.Normal("Glassy linear expansion coefficient", mu=0, sigma =1) #M - G0
        G_1 = pm.Normal("Glassy quadratic expansion coefficient", mu = 0, sigma = 1)
        w = pm.HalfNormal("Transition width")       
        tg = pm.Uniform('Tg', lower = min(x_n), upper = max(x_n)) # ~uniform probability over length of temperature array
    
    
        h_T = w * (M-G_0-G_1)/2 *  pm.math.log(pm.math.cosh((x_n-tg)/w))+(x_n-tg)*(M+G_0)/2 + intercept + G_1/4*((x_n-tg)**2) - G_1/2*np.tanh((x_n-tg)/w)
    
        y_obs = pm.Normal("y_obs", mu = h_T, sigma = sigma, observed=y_n) #likelihood
        trace = pm.sample(draws=draw, tune=tune, init = 'jitter + adapt_diag', step=pm.NUTS(target_accept = target_accept), cores=4, progressbar=TRUE)

    
        if(plot_ppc is True):
            pp = pm.sampling.sample_posterior_predictive(trace, keep_size = True, extend_inferencedata = True)
            prior = pm.sample_prior_predictive(samples=n_count_data)
            az.InferenceData.extend(trace, prior)
            # fig, ax = plt.subplots(figsize=(6,5))  # figure size (x,y) dimensions in inches
            az.plot_ppc(pp, 'cumulative', alpha = .2) #cumulative prob. density for y_obs
            az.plot_ppc(pp) #PDF of y_obs
        summary = az.summary(trace, round_to = 4)
        print(summary)
        now = str(t.ctime())
        if save_data is True:
            np.save("inference_data_" + now, trace)
        if save_summary is True:
            np.save("inference_summary_" +now, summary)
        if return_data is True:
            return trace, count_data, T, h_err
        else:
            return trace

trace, h, T, h_err = lncosh_bayes_fit(fname = filedialog.askopenfilename(multiple = False), 
                               draw = 2000, 
                               tune = 2000, 
                               T_max=140, 
                               save_data = False, 
                               save_summary = False, 
                               plot_ppc = True,
                               fit_error=False)
#show traceplot (parameter value histograms, and value vs. iteration # for each param)
az.plot_trace(trace, figsize=(12, 36))
#compute effective r-squared
y_pred = trace.posterior_predictive.stack(sample=("chain", "draw"))["y_obs"].values.T
y_true = trace.observed_data["y_obs"].values.T

# sigma_posterior = trace.posterior['Sigma'].values.mean()
print(az.r2_score(y_true, y_pred))

#draw pretty figure with posterior predictive + observed data
# fig1, ax1 = plt.subplots(figsize = (9,9))
# fig1.set_dpi(300)
# y_min = np.floor(np.min(h))
# y_max = np.ceil(np.max(h))
# h_observed = trace.observed_data["y_obs"]* np.std(h) + np.mean(h)
# h_ppc = trace.posterior_predictive["y_obs"]* np.std(h) + np.mean(h)
# h_ppc_mean = trace.posterior_predictive["y_obs"].values.mean(axis=0).mean(axis=0)* np.std(h) + np.mean(h)
# predictive_color = 'dodgerblue'
# marker_shape = "^"
# az.plot_lm(
#     y = h_observed, 
#     idata = trace, 
#     y_hat = h_ppc, 
#     x = T, 
#     axes = ax1, 
#     legend = True, 
#     grid = False, 
#     figsize = (8,9), 
#     textsize = 14, 
#     y_kwargs = {"color" : 'black', "ms":7, 'marker':marker_shape, 'mfc':'white', 'label':"Observed data"}, 
#     y_hat_plot_kwargs = {"color":predictive_color, "marker":marker_shape, "ms":7, 'mew':1, 'alpha':1})
# ax1.set_xlabel('Temperature  ($^o$C)', fontsize=15)
# ax1.set_ylabel('Film thickness (nm)', fontsize = 15)
# ax1.set_xlim(25, 145)
# ax1.set_ylim(y_min, y_max)
# ax1.set_yticks(np.arange(y_min, y_max, 0.25), minor = True)
# ax1.set_xticks(range(25, 145, 5), minor = True)
# axt = ax1.secondary_xaxis('top')
# axr = ax1.secondary_yaxis('right')
# axt.set_xticks(range(25, 145, 5), minor = True)
# axr.set_yticks(np.arange(y_min, y_max, 0.25), minor = True)
# axt.tick_params(labeltop = False, direction = 'in', which = "both")
# axr.tick_params(labelright = False, direction = 'in', which = "both")
# ax1.tick_params(axis = 'both', which = 'major', labelsize = 12)
# ax1.plot(T, h_ppc_mean, linestyle = '-', linewidth =2.5, zorder=10, color='red')
##modifying legend entries manually 
# leg = ax1.get_legend()
# leg.legendHandles[1].set_alpha(1)
# leg.legendHandles[1].set_mew(1)


#get 1D array of sampled parameter values from an InferenceData object
def extract_values(trace, parameter):
    param_arr =  trace.posterior[parameter].values
    params = []
    for chain in param_arr:
        for param in chain:
            params.append(param)
    params = np.array(params)
    return params

tgvals = extract_values(trace, "Tg")
tgvals = tgvals * np.std(T) + np.mean(T)

Mvals = extract_values(trace, "Melt linear expansion coefficient")
Gvals = extract_values(trace, "Glassy linear expansion coefficient")
wvals = extract_values(trace, "Transition width")
Ivals = extract_values(trace, "Intercept")
#Pretty Tg distribution plot
paramvals = [Mvals, Gvals, wvals, Ivals]
paramnames  = ["Melt linear expansion coefficient", 
               "Glassy linear expansion coefficient",
              "Transition width",
              "Intercept"]
print("Tg, TgErr,w,werr = " + str(np.mean(tgvals)) + "\t" + str(np.std(tgvals)) + "\t" + str(np.mean(wvals)* np.std(T)) + "\t" + str(np.std(wvals) * np.std(T)))




#plot residual
fig3, ax3 = plt.subplots(figsize=(6,5))
fig3.set_dpi(300)
plt.errorbar(x=T, 
              y=(y_true - y_pred.mean(axis=0))/(h_err/np.std(h)), 
              yerr = np.ones(np.shape(y_true)),
              fmt='o',
              capsize=5,
              mfc='white',
              color='tab:red',
              markersize=7,
              elinewidth=1
              )
ax3.set_xlabel('Temperature  ($^o$C)', fontsize=20)
ax3.set_ylabel("Bayesian Residual")
ax3.set_ylim(-4,4)
ax3.set_xlim(25,145)
ax3t = ax3.secondary_xaxis('top')
ax3r = ax3.secondary_yaxis('right')
ax3.set_xticks(range(25, 145, 5), minor = True, which = "both")
ax3t.set_xticks(range(25, 145, 5), minor = True, which = "both")
ax3.tick_params(axis = 'both', which = 'major', labelsize = 15)

ax3t.tick_params(labeltop = False, direction = 'in', which = "both")
ax3r.tick_params(labelright = False, direction = 'in', which = "both")




